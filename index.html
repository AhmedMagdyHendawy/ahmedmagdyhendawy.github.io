<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ahmed M. Hendawy </title> <meta name="author" content="Ahmed M. Hendawy"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%B9&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="ahmedhendawy.de/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Ahmed</span> M. Hendawy </h1> <p class="desc"><a href="https://www.ias.informatik.tu-darmstadt.de/Team/AhmedHendawy" rel="external nofollow noopener" target="_blank">Ph.D. Candidate</a> at LiteRL and <a href="https://www.ias.informatik.tu-darmstadt.de/" rel="external nofollow noopener" target="_blank">IAS</a> research groups, <a href="https://www.tu-darmstadt.de/" rel="external nofollow noopener" target="_blank">TU Darmstadt</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?580474bbbfbb1a16c28e7733152eee96" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I am a third-year PhD. student at the LiteRL and IAS research groups at TU Darmstadt, Germany. My advisors are Prof. Carlo D’Eramo and Prof. Jan Peters. I have a master’s degree in Information Technology from the University of Stuttgart, with a specialization in Computer Engineering. In 2019, I graduated from the German University in Cairo (GUC) with a bachelor’s degree in Mechatronics Engineering.</p> <p>My research delves into the Reinforcement Learning topics of multi-task and continual learning. My research objective is to boost the learning process of agents by leveraging knowledge from multiple tasks learned concurrently or in sequence. I develop RL algorithms that are application agnostic; however, I lean towards robot learning applications.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Aug 21, 2024</th> <td> Our survey “Machine Learning with Physics Knowledge for Prediction: A Survey” is out. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 17, 2024</th> <td> Our work “Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts” has been accepted at ICLR 2024 for a poster presentation. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 22, 2023</th> <td> Our work “Using Proto-Value Functions for Curriculum Generation in Goal-Conditioned RL” has been accepted at the NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 18, 2023</th> <td> Our work “Parameter-efficient Tuning of Pretrained Visual-Language Models in Multitask Robot Learning” has been accepted at the CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP). </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 16, 2023</th> <td> I was selected as one of the <a href="https://aistats.org/aistats2023/reviewers.html" rel="external nofollow noopener" target="_blank">top 10% of reviewers</a> who volunteered in AISTATS 2023. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 16, 2022</th> <td> My master’s thesis “Constraint-based Optimization Approach for Generalized Few-Shot Object Detection” was awarded the Sony Award for best master’s thesis at the University of Stuttgart. </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Aug 09, 2020</th> <td> <a class="news-title" href="/blog/2020/part3-imageclassification/">Part 3 - Model Testing — Image Classification with YonoHub &amp; Tensorflow V2.0 Series</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 05, 2020</th> <td> <a class="news-title" href="/blog/2020/part2-imageclassification/">Part 2 - Model Implementation &amp; Training — Image Classification with YonoHub &amp; Tensorflow V2.0</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 01, 2020</th> <td> <a class="news-title" href="/blog/2020/sumo/">SUMO — a Traffic Simulator over the Cloud with Yonohub</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/physicsMLSurvey-480.webp 480w,/assets/img/publication_preview/physicsMLSurvey-800.webp 800w,/assets/img/publication_preview/physicsMLSurvey-1400.webp 1400w," sizes="1024px" type="image/webp"> <img src="/assets/img/publication_preview/physicsMLSurvey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="physicsMLSurvey.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="physicsMLSurvey" class="col-sm-8"> <div class="title">Machine Learning with Physics Knowledge for Prediction: A Survey</div> <div class="author"> Joe Watson ,  Chen Song ,  Oliver Weeger , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Theo Gruner, An T Le, Kay Hansel, Ahmed Hendawy, Oleg Arenz, Will Trojak, Miles Cranmer, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2408.09840</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>This survey examines the broad suite of methods and models for combining machine learning with physics knowledge for prediction and forecast, with a focus on partial differential equations. These methods have attracted significant interest due to their potential impact on advancing scientific research and industrial practices by improving predictive models with small- or large-scale datasets and expressive predictive models with useful inductive biases. The survey has two parts. The first considers incorporating physics knowledge on an architectural level through objective functions, structured predictive models, and data augmentation. The second considers data as physics knowledge, which motivates looking at multi-task, meta, and contextual learning as an alternative approach to incorporating physics knowledge in a data-driven fashion. Finally, we also provide an industrial perspective on the application of these methods and a survey of the open-source ecosystem for physics-informed machine learning. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">physicsMLSurvey</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2408.09840}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Machine Learning with Physics Knowledge for Prediction: A Survey}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Watson, Joe and Song, Chen and Weeger, Oliver and Gruner, Theo and Le, An T and Hansel, Kay and Hendawy, Ahmed and Arenz, Oleg and Trojak, Will and Cranmer, Miles and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2408.09840}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/moore-480.webp 480w,/assets/img/publication_preview/moore-800.webp 800w,/assets/img/publication_preview/moore-1400.webp 1400w," sizes="1024px" type="image/webp"> <img src="/assets/img/publication_preview/moore.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="moore.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="moore" class="col-sm-8"> <div class="title">Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</div> <div class="author"> <em>A. Hendawy</em> ,  J. Peters ,  and  C. D’Eramo </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">moore</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2311.11385.pdf}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hendawy, A. and Peters, J. and D'Eramo, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/corl_leap_paper-480.webp 480w,/assets/img/publication_preview/corl_leap_paper-800.webp 800w,/assets/img/publication_preview/corl_leap_paper-1400.webp 1400w," sizes="1024px" type="image/webp"> <img src="/assets/img/publication_preview/corl_leap_paper.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="corl_leap_paper.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="corl_leap_paper" class="col-sm-8"> <div class="title">Parameter-efficient Tuning of Pretrained Visual-Language Models in Multitask Robot Learning</div> <div class="author"> M. Mittenbuehler ,  <em>A. Hendawy</em> ,  C. D’Eramo , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'G. Chalvatzaki' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Multimodal pretrained visual-language models (pVLMs) have showcased excellence across several applications, like visual question-answering. Their recent application for policy learning manifested promising avenues for augmenting robotic capabilities in the real world. This paper delves into the problem of parameter-efficient tuning of pVLMs for adapting them to robotic manipulation tasks with low-resource data. We showcase how Low-Rank Adapters (LoRA) can be injected into behavioral cloning temporal transformers to fuse language, multi-view images, and proprioception for multitask robot learning, even for long-horizon tasks. Preliminary results indicate our approach vastly outperforms baseline architectures and tuning methods, paving the way toward parameter-efficient adaptation of pretrained large multimodal transformers for robot learning with only a handful of demonstrations. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">corl_leap_paper</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/pdf?id=UsY1YvsPzK}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mittenbuehler, M. and Hendawy, A. and D'Eramo, C. and Chalvatzaki, G.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Parameter-efficient Tuning of Pretrained Visual-Language Models in Multitask Robot Learning}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/crpi-480.webp 480w,/assets/img/publication_preview/crpi-800.webp 800w,/assets/img/publication_preview/crpi-1400.webp 1400w," sizes="1024px" type="image/webp"> <img src="/assets/img/publication_preview/crpi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="crpi.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="crpi" class="col-sm-8"> <div class="title">Using Proto-Value Functions for Curriculum Generation in Goal-Conditioned RL</div> <div class="author"> H. Metternich ,  <em>A. Hendawy</em> ,  P. Klink , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'J. Peters, C. D’Eramo' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In this paper, we investigate the use of Proto Value Functions (PVFs) for measuring the similarity between tasks in the context of Curriculum Learning (CL). PVFs serve as a mathematical framework for generating basis functions for the state space of a Markov Decision Process (MDP). They capture the structure of the state space manifold and have been shown to be useful for value function approximation in Reinforcement Learning (RL). We show that even a few PVFs allow us to estimate the similarity between tasks. Based on this observation, we introduce a new algorithm called Curriculum Representation Policy Iteration (CRPI) that uses PVFs for CL, and we provide a proof of concept in a Goal-Conditioned Reinforcement Learning (GCRL) setting. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">crpi</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/pdf?id=DYd2q5wufn}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Metternich, H. and Hendawy, A. and Klink, P. and Peters, J. and D'Eramo, C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using Proto-Value Functions for Curriculum Generation in Goal-Conditioned RL}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{NeurIPS 2023 Workshop on Goal-Conditioned Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">L3D-IVU</abbr> </div> <div id="cfa" class="col-sm-8"> <div class="title">CFA: Constraint-based Finetuning Approach for Generalized Few-Shot Object Detection</div> <div class="author"> K. Guirguis ,  <em>A. Hendawy</em> ,  G. Eskandar , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'M. Abdelsamad, M. Kayser, J. Beyerer' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em></em> 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Few-shot object detection (FSOD) seeks to detect novel categories with limited data by leveraging prior knowledge from abundant base data. Generalized few-shot object detection (G-FSOD) aims to tackle FSOD without forgetting previously seen base classes and, thus, accounts for a more realistic scenario, where both classes are encountered during test time. While current FSOD methods suffer from catastrophic forgetting, G-FSOD addresses this limitation yet exhibits a performance drop on novel tasks compared to the state-of-the-art FSOD. In this work, we propose a constraint-based finetuning approach (CFA) to alleviate catastrophic forgetting, while achieving competitive results on the novel task without increasing the model capacity. CFA adapts a continual learning method, namely Average Gradient Episodic Memory (A-GEM) to G-FSOD. Specifically, more constraints on the gradient search strategy are imposed from which a new gradient update rule is derived, allowing for better knowledge exchange between base and novel classes. To evaluate our method, we conduct extensive experiments on MS-COCO and PASCAL-VOC datasets. Our method outperforms current FSOD and G-FSOD approaches on the novel task with minor degeneration on the base task. Moreover, CFA is orthogonal to FSOD approaches and operates as a plug-and-play module without increasing the model capacity or inference time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cfa</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2204.05220}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Guirguis, K. and Hendawy, A. and Eskandar, G. and Abdelsamad, M. and Kayser, M. and Beyerer, J.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CFA: Constraint-based Finetuning Approach for Generalized Few-Shot Object Detection}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Workshop on Learning with Limited Labelled Data for Image and Video Understanding (L3D-IVU)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{arXiv.org perpetual, non-exclusive license}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%68%6D%65%64%6D%61%67%64%79%61%68%6D%65%64%31%39%39%36@%6F%75%74%6C%6F%6F%6B.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-8088-3004" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=nwW0K8UAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/AhmedMagdyHendawy" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/ahmed-magdy-hendawy" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/AHendawy19" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://medium.com/@ahmedmagdyattia1996" title="Medium" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-medium"></i></a> <a href="https://www.ias.informatik.tu-darmstadt.de/Team/AhmedHendawy" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> <a href="https://gitlab.com/ahmedhendawy" title="GitLab" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-gitlab"></i></a> </div> <div class="contact-note">Better reach me via email. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ahmed M. Hendawy. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>