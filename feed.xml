<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="ahmedhendawy.de/feed.xml" rel="self" type="application/atom+xml"/><link href="ahmedhendawy.de/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-26T17:06:38+00:00</updated><id>ahmedhendawy.de/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Part 3 - Model Testing — Image Classification with YonoHub &amp;amp; Tensorflow V2.0 Series</title><link href="ahmedhendawy.de/blog/2020/part3-imageclassification/" rel="alternate" type="text/html" title="Part 3 - Model Testing — Image Classification with YonoHub &amp;amp; Tensorflow V2.0 Series"/><published>2020-08-09T16:40:16+00:00</published><updated>2020-08-09T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2020/part3-imageclassification</id><content type="html" xml:base="ahmedhendawy.de/blog/2020/part3-imageclassification/"><![CDATA[<p align="center"> <a href="https://algotrading101.com/learn/what-is-overfitting-in-trading/"><img src="https://cdn-images-1.medium.com/max/2000/1*LC0B80A8Y6Y-YjxQ-J5MbQ.png" width="800px" title="Tensorflow Series" alt="Tensorflow Series"/></a> </p> <p>Developing effective deep learning is not an easy task. The deep learning researchers are competing to benchmark their models against many datasets. One can design an object detection model for autonomous driving and benchmark it against Kitti, nuScenes, Waymo, etc.… Another one creates an image classification model and makes sure to achieve high performance against COCO, ImageNet, etc.… But why is that?</p> <p>Actually, it is not for a single reason; however, there is a motive, which drives all these eager researchers, which is reaching the generality of their model. To have a model that is not locally optimally according to the trained data provided is the target.</p> <p>Various problems need to be tackled to reach such a performance. One common challenge which is famous in the machine learning/ deep learning community is the overfitting. Your model is too complicated to the extend of memorizing all the answers from your training dataset.</p> <p>Many solutions have been provided to solve such an issue, starting from having much bigger data to regulate your training loop. However, a first step before solving the problem is to know how to detect it. As the chief needs to taste their food before serving, the deep learning developer needs to check the overfitting before serving the model as a solution for the targeted problem. Testing the model on a separated unseen dataset is the way to do so. If your model performs on the test dataset roughly similar, in terms of accuracy, on the training one, you are ready to go.</p> <p>This is why in this tutorial we wanted to cover how to do testing using a test split unseen by our trained model. Using the same pipeline from the previous <a href="https://medium.com/yonohub/part-2-model-implementation-training-image-classification-with-yonohub-tensorflow-v2-0-f74e72878e77">tutorial</a> with some slight changes discussed below to validate our model and check if there is overfitting or not.</p> <p>In this series of tutorials, we will go through a deep learning journey, especially for <strong>Image Classification</strong>, starting from streaming a dataset till the deployment. The tutorials cover how to use Tensorflow in Yonohub by using the blocks offered within YonoArc for a fast and easy way of development and deployment.</p> <h2 id="upgrading-the-model-yonoarc-block">Upgrading the Model YonoArc block</h2> <p>The training loop in YonoArc is divided into a dataset player and a model. Each implemented and encapsulated within a block. Similar to the previous tutorial the dataset player publishes the CIFAR-10 dataset continuously in batches with a specific frame rate. Previously, we chose the train split, however, this time a test split will be selected instead.</p> <p>The model block was by default operated in a training mode. A droplist is provided to select the operating mode of the model either for training or testing. For the newly added testing mode, we add similar functions for the block’s source code, <em>testing</em>, and <em>test_step</em> function as shown below,</p> <script src="https://gist.github.com/AhmedMagdyHendawy/eeccf2cc2ac39fa5571bbd546652b258.js"></script> <p>The functions are similar to the normal functions used by TensorFlow in addition to some pieces of code to publish the results to the user in terms of an INFO alert as well as port messages. We used the model parameters trained from the previous tutorial to be loaded and ready for testing.</p> <h2 id="tensorflow-testing-in-yonohub">TensorFlow Testing in Yonohub</h2> <p>You can follow the next video tutorial to replicate the steps required to perform the testing loop. However, you can download the .arc file of the pipeline and open it in YonoArc, from <a href="https://github.com/YonoHub/Model-Testing">here</a>, directly without worrying about even setting the above parameters. Although, you need to freely purchase all the blocks used in the article from YonoStore.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=qDcUm8kyZQE"><img src="http://img.youtube.com/vi/qDcUm8kyZQE/0.jpg" width="800px" title="Tensorflow Testing in Yonohub " alt="Tensorflow Testing in Yonohub "/></a> </p> <p align="center"> Tensorflow Testing in Yonohub </p> <h2 id="conclusion">Conclusion</h2> <p>As the above video tutorial demonstrates, the percentage of the testing is approximately 71% which is very close to training accuracy from the previous tutorial. Now, our model is ready to be deployed on hardware to be tested in a real-life scenario.</p> <p>Using Yonoarc for testing facilitates the idea of validating your model against different real-life datasets by just replacing the Image Classification Dataset Player by any other dataset player to cover a larger distribution of data during testing of your trained model.</p> <h2 id="about-yonohub">About Yonohub</h2> <blockquote> <p><em>Yonohub is a web-based cloud system for development, evaluation, integration, and deployment of complex systems, including Artificial Intelligence, Autonomous Driving, and Robotics. Yonohub features a drag-and-drop tool to build complex systems, a marketplace to share and monetize blocks, a builder for custom development environments, and much more. YonoHub can be deployed on-premises and on-cloud.</em></p> </blockquote> <p>Get $25 free credits when you sign up now. For researchers and labs, contact us to learn more about Yonohub sponsorship options.</p> <p>If you liked this article, please consider following us on Twitter at <a href="https://twitter.com/YonoHub">@yonohub</a>, <a href="mailto:info@yonohub.com">email us directly</a>, or <a href="https://www.linkedin.com/showcase/yonohub">find us on LinkedIn</a>. I’d love to hear from you if I can help you or your team with how to use Yonohub.</p> <h2 id="references">References</h2> <p>[1] <a href="https://www.tensorflow.org/tutorials/images/cnn">https://www.tensorflow.org/tutorials/images/cnn</a></p> <h2 id="references-1">References</h2> <p>[1] <a href="https://www.tensorflow.org/tutorials/images/cnn">https://www.tensorflow.org/tutorials/images/cnn</a></p>]]></content><author><name></name></author><category term="Yonohub"/><category term="Image_Classification"/><category term="Tensorflow"/><summary type="html"><![CDATA[Developing effective deep learning is not an easy task. The deep learning researchers are competing to benchmark their models against many datasets. One can design an object detection model for…]]></summary></entry><entry><title type="html">Part 2 - Model Implementation &amp;amp; Training — Image Classification with YonoHub &amp;amp; Tensorflow V2.0</title><link href="ahmedhendawy.de/blog/2020/part2-imageclassification/" rel="alternate" type="text/html" title="Part 2 - Model Implementation &amp;amp; Training — Image Classification with YonoHub &amp;amp; Tensorflow V2.0"/><published>2020-07-05T16:40:16+00:00</published><updated>2020-07-05T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2020/part2-imageclassification</id><content type="html" xml:base="ahmedhendawy.de/blog/2020/part2-imageclassification/"><![CDATA[<p align="center"> <img src="https://miro.medium.com/max/700/1*dnhwMfBfE39B5h2d4NmCgQ.gif" width="800px" title="Tensorflow Series" alt="Tensorflow Series"/> </p> <p>Researches, in the Deep Learning community, are developing state-of-the-art algorithms to tackle various problems in our daily life. Starting from simple Cat-Dog image classifier to Facebook <a href="https://arxiv.org/pdf/2006.03511.pdf">TransCoder</a> AI Model which translates one programming language to another.</p> <p>Many challenges are facing researchers in the field. One of them is to collect the appropriate dataset for the problem settings. However, as we discussed in the <a href="https://medium.com/yonohub/part-1-introducing-tensorflow-datasets-in-yonohub-suit-image-classification-with-yonohub-cdf44649a223">previous</a> tutorial of this series, we have a new stream of data every day. The trick is to easily reuse the dataset with different models written in different frameworks without the compatibility issue. We covered in the tutorial how we are using a single format of all the dataset, using the Tensorflow Dataset package, in a single <a href="https://yonohub.com/yonoarc/">YonoArc</a> block to be the higher level of the training stack with which the users can interact without writing a single line of code.</p> <p>Creating the architecture of the model is another challenge but it is mainly the whole point of research. Researchers are competing to reach the best performance in their area of research.</p> <p>To reach such a high performance, you need a fast and easy way to create, train, and test the proposed architecture. Intuitively, we can make use of the easy drag and drop YonoArc block with 50+ datasets from the previous tutorial.</p> <p>In this series of tutorials, we will go through a deep learning journey, especially for <strong>Image Classification</strong>, starting from streaming a dataset till the deployment. The tutorials cover how to use Tensorflow in Yonohub by using the blocks offered within YonoArc for a fast and easy way of development and deployment.</p> <p>In this tutorial, we will implement a CNN classifier model for the CIFAR-10 dataset streamed from the Tensorflow Dataset Player implemented in the last tutorial. Then, we train the model and log the Loss as well as the Accuracy of the model to the Dashboard in YonoArc. During training, we use <a href="https://github.com/lutzroeder/netron">Netron</a> encapsulated as a block to visualize the architecture of the implemented model. Moreover, the CPU/ GPU consumption is checked using SSH communication to the model block. Finally, we reviewed the results of the training.</p> <h2 id="model-implementation">Model Implementation</h2> <p>Imagine that the traditional training loop is split into different blocks of code that have a standard way of communication. Let’s split, as a start, the loop into a dataset player and a model. If we have a common way of sending and receiving the data between the player and the model. We can have a reusable training loop in the sense that we can replace different models on the same dataset or vice versa. This is the main point of using Yonohub for training.</p> <p>Let us design the second component in the new training loop!</p> <p>To have more freedom during the training loop design, we preferred using the custom training in Tensorflow.</p> <p>As shown below, the <em>cifar_cnn</em> class represents the CIFAR CNN block object in YonoArc which you can freely purchase from <a href="https://store.yonohub.com/product/cifar-cnn/">here</a>. The constructor contains the initialization of the default values for the class attributes, for example, the available classes in the CIFAR-10 dataset, the mapping of the classes to integer values, etc…</p> <script src="https://gist.github.com/AhmedMagdyHendawy/a740cd1da61024feea606bb4e541e787.js"></script> <p>In <em>on_start(self)</em>, we get the values of some essential parameters, from the user, using <em>get_property</em> function, for example, the momentum, learning rate, number of epochs, etc…</p> <script src="https://gist.github.com/AhmedMagdyHendawy/6639896d973c8510bfc5310ad8d54aa5.js"></script> <p>The CIFAR model is created as well in the <em>on_start(self)</em> by calling the <em>create_model(self)</em>. This function constructs a sequential model of layers as shown above.</p> <script src="https://gist.github.com/AhmedMagdyHendawy/84732bfb14aee0a603043c41caff2f81.js"></script> <p>At the end of <em>on_start(self)</em>, some necessary objects are created, for example, the optimizer as well as the loss and the accuracy objects.</p> <p>After <em>on_start(self)</em>, <em>run(self) *function is called in parallel to the *on_new_messages *function. The *run(self)</em> is responsible for the training loop while reading the images and labels from the <em>self.batches</em> queue which contains the available data batched using the <em>on_new_messages</em> function.</p> <script src="https://gist.github.com/AhmedMagdyHendawy/676dcec97966d9841acf3cfebb763eb2.js"></script> <script src="https://gist.github.com/AhmedMagdyHendawy/7b3a247b16e4cfa4c3a9f77b1bd52d3e.js"></script> <p>From the previous tutorial, we had a player block that publishes the image classification dataset as a single image and label. In this tutorial, we changed the player block to publish a batch of images as well as the corresponding labels. You can freely purchase the new player from <a href="https://store.yonohub.com/product/image-classification-batch-player/">YonoStore</a>.</p> <p>The <em>training(self)</em> function is called in the <em>run(self)</em> to conduct the training. The training(self) function is a custom Tensorflow implementation of the training loop, however, the data is read from the <em>self.batches</em> queue with a preprocessing step done by the batch_transform(self) function.</p> <script src="https://gist.github.com/AhmedMagdyHendawy/0c7d51808bc02aa5eb8a28c0e9c28637.js"></script> <p>The <em>batch_transform(self)</em> function converts the ROS messages, which is the format of messages in YonoArc, to Tensors. During the conversion, the image batch is converted to RGB from BGR format due to the previous conversion in the player block using OpenCV. The label batch is mapped to the integer version using the <em>self.ind2label</em> dictionary defined in the block constructor. Before fitting the data to the model, the images are normalized.</p> <script src="https://gist.github.com/AhmedMagdyHendawy/c5878867c75c4f9cc62f2ee1074067dc.js"></script> <p>A normal training loop is conducted by calling the <em>train_step(self)</em> function which conducts the forward feeding as well as the backpropagation. It calculates the loss as well as the accuracy. We save the model at the end of each epoch which will be used for visualization of the ANN architecture using Netron later. The model is saved as a .h5 file with a filename given by the user and it’s location too.</p> <script src="https://gist.github.com/AhmedMagdyHendawy/061c3c6583446ef8e0964eda9877218b.js"></script> <p>Now, let us demonstrate how to close the training loop by connecting the player to the CIFAR CNN block model described above. Moreover, we can see how we use the YonoArc interface to set all the required parameters.</p> <h2 id="tensorflow-training-in-yonohub">Tensorflow Training in Yonohub</h2> <p>The training pipeline in YonoArc is easier to be understood. A block that streams the data (images, labels) with a specific frame rate you can choose. The dataset can be selected from a drop list in the block which includes 50+ image classification datasets offered by the Tensorflow Dataset package.</p> <p>Another block that represents the model which are going to be trained on the selected dataset. Both blocks are connected using two ports through which both the batch of images as well as the labels are exchanged. The batch size can be set by creating a global parameter in the pipeline and both blocks can read it easily.</p> <p>The model block publishes two values each epoch: Loss and Accuracy. Both values can be visualized using the Line Charts block which plots the data live to the dashboard.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2736/1*uIDeOmzS4TI7eNDGl45jGA.png" width="800px" title="YonoArc Training Pipeline" alt="YonoArc Training Pipeline"/> </p> <p align="center"> YonoArc Training Pipeline </p> <p>Now, we are ready to launch our training pipeline. The player block will instantly download the CIFAR-10 dataset we select. After the downloading is complemented an alert will be sent to state so. You can click the play button which is newly added to this player block to let the user determine when to start the training. Consequently, the data starts to be streamed between the two blocks and after a while, the model block will give an alert of the current epoch. You can continuously check the dashboard for the progress of the training.</p> <h2 id="visualize-the-ann-architecture">Visualize the ANN Architecture</h2> <p>While the training is in progress, the ANN architecture can be visualized using the Netron block. Netron is a viewer for neural networks, deep learning, and machine learning models. We encapsulate the Netron package in a YonoArc block to easily drag and drop while training.</p> <p>After placing the block in the pipeline, the location of the model.h5 is required. Then, we can launch the block and wait for the running mode.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2736/1*fbFUMGsfum2u_Uy4b-vbzQ.png" width="800px" title="Running Training Pipeline" alt="Running Training Pipeline"/> </p> <p align="center"> Running Training Pipeline </p> <p>A clickable URL will be appeared in the block settings to be used to open Netron web-app in a new tab. The model architecture is shown in a nice and a readable way as seen below,</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2720/1*x6rLMcw8QyLn0halcqZ4-w.png" width="800px" title="Model Architecture visualized in Netron" alt="Model Architecture visualized in Netron"/> </p> <p align="center"> Model Architecture visualized in Netron </p> <h2 id="cpu-gpu-consumption">CPU/ GPU Consumption</h2> <p>Still, there is one missing part of an effective training loop. The efficiency of the developed algorithm can be continuously reviewed while the training is in progress. We added an SSH communication channel in the model block to let users have access to the machine at which the model runs.</p> <p>Two properties are added to the model block to establish this channel. A username and a password for the user of the block. Yet, you can use the default values of the properties. The two entities are set before the launch of the pipeline.</p> <p>From your local terminal, you can type the following command,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo ssh [username]@[URL] -p [port]
</code></pre></div></div> <p>You need to replace the [username]by the value, you inserted in the block property. [URL] as well as [port]can be replaced by the URL: Port which has shown in the model block’s settings.</p> <p>After inserting the password, you have access to the GPU machine used for training the model. You can run multiple useful commands to check the CPU as well as the GPU consumption. For the CPU consumption, you can use the top/ htop command as shown below,</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2000/1*VCOBJqwzpgb8lpwcq1mY9A.png" width="800px" title="htop" alt="htop"/> </p> <p align="center"> htop </p> <p>In addition, you can use the nvidia-smi command to review the GPU consumption,</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2000/1*nWoHSKrXzd3NRyzdlpHfzQ.png" width="800px" title="nvidia-smi" alt="nvidia-smi"/> </p> <p align="center"> nvidia-smi </p> <h2 id="check-training-results">Check Training Results</h2> <p>Finally, we can check the final results of the training of our CIFAR classifier. The model block sends alerts indicating the completion of the training process as well as saving the final model. The loss and accuracy of the model, over the epochs, can be viewed from the dashboard as shown below,</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2670/1*PoaoMc_s45J5Qg9ddxaM5A.png" width="800px" title="Loss and Accuracy" alt="Loss and Accuracy"/> </p> <p align="center"> Loss and Accuracy </p> <h2 id="training-in-yonohub">Training in Yonohub</h2> <p>You can follow the next video tutorial to replicate the steps required to produce the above results. However, you can download the .arc file of the pipeline and open it in YonoArc, from <a href="https://github.com/YonoHub/Model-Implementation-and-Training">here</a>, directly without worrying about even setting the above parameters. Although, you need to freely purchase all the blocks used in the article from YonoStore.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=YPVWdqRs320"><img src="http://img.youtube.com/vi/YPVWdqRs320/0.jpg" width="800px" title="Visualize the ANN Architecture" alt="Visualize the ANN Architecture"/></a> </p> <p align="center"> Visualize the ANN Architecture </p> <h2 id="conclusion">Conclusion</h2> <p>Now, you have a trained classifier on the CIFAR-10 dataset using a new way of training. YonoArc facilitates the idea of training by using the concept of divide and conquer. Separating the player and the model in two distinct blocks makes the training loop more reusable later. It facilitates the idea of inserting different utility packages like Netron while the training is in progress.</p> <p>More benefits can be made from the above training setup. Many utility packages can be easily integrated. In the rest of the series, we will point out some of these benefits. We are waiting for your first experience with the new setup and your feedback.</p> <h2 id="about-yonohub">About Yonohub</h2> <blockquote> <p><em>Yonohub is a web-based cloud system for development, evaluation, integration, and deployment of complex systems, including Artificial Intelligence, Autonomous Driving, and Robotics. Yonohub features a drag-and-drop tool to build complex systems, a marketplace to share and monetize blocks, a builder for custom development environments, and much more. YonoHub can be deployed on-premises and on-cloud.</em></p> </blockquote> <p>Get $25 free credits when you sign up now. For researchers and labs, contact us to learn more about Yonohub sponsorship options.</p> <p>If you liked this article, please consider following us on Twitter at <a href="https://twitter.com/YonoHub">@yonohub</a>, <a href="mailto:info@yonohub.com">email us directly</a>, or<a href="https://www.linkedin.com/showcase/yonohub"> find us on LinkedIn</a>. I’d love to hear from you if I can help you or your team with how to use Yonohub.</p> <h2 id="reference">Reference</h2> <p>[1] <a href="https://venturebeat.com/2020/06/08/facebooks-transcoder-ai-converts-code-from-one-programming-language-into-another/">https://venturebeat.com/2020/06/08/facebooks-transcoder-ai-converts-code-from-one-programming-language-into-another/</a></p> <p>[2] <a href="https://arxiv.org/pdf/2006.03511.pdf">https://arxiv.org/pdf/2006.03511.pdf</a></p>]]></content><author><name></name></author><category term="Yonohub"/><category term="Image_Classification"/><category term="Tensorflow"/><summary type="html"><![CDATA[Researches, in the Deep Learning community, are developing state-of-the-art algorithms to tackle various problems in our daily life. Starting from simple Cat-Dog image classifier to Facebook…]]></summary></entry><entry><title type="html">SUMO — a Traffic Simulator over the Cloud with Yonohub</title><link href="ahmedhendawy.de/blog/2020/sumo/" rel="alternate" type="text/html" title="SUMO — a Traffic Simulator over the Cloud with Yonohub"/><published>2020-06-01T16:40:16+00:00</published><updated>2020-06-01T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2020/sumo</id><content type="html" xml:base="ahmedhendawy.de/blog/2020/sumo/"><![CDATA[<p align="center"> <img src="https://miro.medium.com/max/700/1*RLi4jgINeNszuFafJ-vnpQ.png" width="800px" title="SUMO" alt="SUMO"/> </p> <p align="center"> SUMO </p> <p>Intelligent Transportation System is an essential building block of full autonomy on the roads. To have a safe, sustainable, and efficient autonomous system in our daily life, a complex traffic system should be deployed on our normal roads. Vehicles should communicate with each other as well as with the road to send and receive some information.</p> <p>This can be exemplified by assuming that we have a traffic jam in one of the intersections in the city. If we have a standard traffic lighting system, we can not normally solve the problem. On the other hand, assume that we have an intelligent system that interacts with the vehicles by receiving their locations and speed, the system at that point can change the traffic light sequence timing to resolve that issue. But how can we implement such an algorithm in the first place? Are we going to test it directly on the road?</p> <p>Now, the traffic simulation comes to place. It facilitates the implementation, evaluation, and testing of such algorithms. However, it is not easy to find such a comprehensive simulator. But we have <a href="https://sumo.dlr.de/docs/">SUMO</a>!</p> <h2 id="sumo">SUMO</h2> <blockquote> <p><strong>“S</strong>imulation of <strong>U</strong>rban <strong>MO</strong>bility” (Eclipse SUMO) is an open-source, highly portable, microscopic, and continuous road traffic simulation package designed to handle large road networks.</p> </blockquote> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2560/1*l8m3a4PnxCnOlmOUkEtNhw.gif" width="800px" title="SUMO Scenario" alt="SUMO Scenario"/> </p> <p align="center"> SUMO Scenario </p> <h3 id="what-features-offered-by-sumo">What features offered by SUMO?</h3> <ul> <li> <p>Microscopic simulation — vehicles, pedestrians and public transport are modeled explicitly</p> </li> <li> <p>Online interaction — control the simulation with <a href="https://sumo.dlr.de/docs/TraCI.html">TraCI</a></p> </li> <li> <p>Simulation of multimodal traffic, e.g., vehicles, public transport, and pedestrians</p> </li> <li> <p>Time schedules of traffic lights can be imported or generated automatically by SUMO</p> </li> <li> <p>No artificial limitations in network size and the number of simulated vehicles</p> </li> <li> <p>Supported import formats: <a href="https://sumo.dlr.de/docs/Networks/Import/OpenStreetMap.html">OpenStreetMap</a>, <a href="https://sumo.dlr.de/docs/Networks/Import/VISUM.html">VISUM</a>, <a href="https://sumo.dlr.de/docs/Networks/Import/Vissim.html">VISSIM</a>, <a href="https://sumo.dlr.de/docs/Networks/Import/DlrNavteq.html">NavTeq</a></p> </li> <li> <p>SUMO is implemented in C++ and uses only portable libraries</p> </li> </ul> <p>SUMO is a powerful simulator for researchers, developers, and enthusiasts. Two ways are normally used to use any simulator. Either by running it on a local machine or over the cloud. Sadly, running the simulator locally is a bottleneck for many developers with any powerful simulator due to the available computational power. Fortunately, we can use <a href="https://yonohub.com/">Yonohub</a>.</p> <h2 id="how-to-integrate-sumo-in-yonohub">How to integrate SUMO in Yonohub?</h2> <p>Not only the computational power in Yonohub we want to get benefit from, however the encapsulation concept in <a href="https://yonohub.com/yonoarc/">YonoArc</a> by designing your simulation scenario as a pipeline with different pieces of code encapsulated in blocks.</p> <h3 id="sumo-environment">SUMO Environment</h3> <p>Normally step zero will be the installation of SUMO. Yonohub offers <a href="https://yonohub.com/yonoebuilder/">YonoEBuilder</a> which facilitates the steps of creating environments with few clicks. You can reuse and share the environments easily. For SUMO, you can even forget step zero as we published an <a href="https://store.yonohub.com/product/sumo-environment/">environment</a>, which contains the latest version of SUMO, in <a href="https://yonohub.com/yonostore/">YonoStore</a>. Here is a view of the SUMO environment developed using YonoEBuilder,</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2000/1*rEw8V8a8xSJhM4y6vakwTg.png" width="800px" title="SUMO Environment" alt="SUMO Environment"/> </p> <p align="center"> SUMO Environment </p> <p>As we can notice, the environment installs SUMO, clones the official repository, and setups <a href="https://novnc.com/info.html">noVNC</a> to visualize SUMO.</p> <p><strong>NOTE</strong>: You can use this environment in a <a href="https://docs.yonohub.com/docs/yonohub/custom-apps/">custom app</a> in the phase of creating a network in <a href="https://sumo.dlr.de/docs/NETEDIT.html">NETEDIT</a> as well as interacting with SUMO-GUI directly.</p> <h3 id="scenario">Scenario</h3> <p>Traffic Light System (TLS) is the scenario integrated. We used the pre-implemented TLS <a href="https://github.com/eclipse/sumo/tree/master/tests/complex/tutorial/traci_tls">example</a> in SUMO. The example uses the TraCI API to interact with SUMO via a client-server protocol, where SUMO acts as the server and TraCI as the client.</p> <h3 id="sumo-yonoarc-block">SUMO YonoArc Block</h3> <p>Next, we use that environment to develop a YonoArc block. We used the Block Manager to create the YonoArc block. The SUMO block encapsulates the following functionalities,</p> <ul> <li> <p>Running noVNC in a background process.</p> </li> <li> <p>Testing the above TLS scenario.</p> </li> <li> <p>Streaming some data related to the simulation to be visualized through the Dashboard.</p> </li> </ul> <p>To implement the above functionalities, we created the SUMO block using the <a href="https://docs.yonohub.com/docs/yonohub/yonoarc/yonoarc-python3-api/">YonoArc Python 3 API</a>. The python 3 source code of the block is an integration of the Python 3 API and the TLS <a href="https://github.com/eclipse/sumo/blob/master/tests/complex/tutorial/traci_tls/runner.py">example</a> as shown below,</p> <script src="https://gist.github.com/AhmedMagdyHendawy/7c52f4c503e97a1b38f3e17e0d81f66b.js"></script> <p>In the <em>on_start(self)</em> function, we run noVNC as a separate process. Then, generating the routes and randomly assign 3600 vehicles to the created routes, using <em>generate_routefile()</em> function. Next, we start SUMO using the <em>start</em> function in TraCI which establishes as well the client-server communication.</p> <p>After that, the <em>run(self)</em> function is called. The function simply calls another function called <em>run_scenario(self)</em> which is responsible for the changing of the traffic light sequence as well as publish the following data related to one of the vehicles,</p> <ul> <li> <p>Position</p> </li> <li> <p>Speed</p> </li> <li> <p>Acceleration</p> </li> <li> <p>Lateral Speed</p> </li> <li> <p>Waiting Time</p> </li> </ul> <p>All these data are published through the block’s ports. All the information is extracted and streamed each step of the simulation to be visualized in YonoArc via the Dashboard.</p> <p>Moreover, we need to add the noVNC URL to the block during creation to be able of opening noVNC in YonoArc.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2216/1*I6VmkFOPKRaknQ-z47E-nw.png" width="800px" title="URL Insertion" alt="URL Insertion"/> </p> <p align="center"> URL Insertion </p> <p>Now, you successfully create the SUMO YonoArc block and ready to construct the pipeline in YonoArc.</p> <h3 id="how-to-use-sumo-in-yonohub">How to Use SUMO in Yonohub?</h3> <p>Let us demonstrate the utilization of the SUMO block in the following video tutorial by forming a pipeline which runs the TLS scenario and visualize some data.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=7-fWAOZOP18"><img src="http://img.youtube.com/vi/7-fWAOZOP18/0.jpg" width="800px" title="SUMO TLS Tutorial in Yonohub" alt="SUMO TLS Tutorial in Yonohub"/></a> </p> <p align="center"> SUMO TLS Tutorial in Yonohub </p> <p>All the <a href="https://store.yonohub.com/product/sumo/">blocks</a> you can freely purchase it from YonoStore. You can download the arc file of the tutorial above directly from <a href="https://github.com/YonoHub/SUMO---a-Traffic-Simulator-over-the-Cloud-with-YonoHub">here</a> as well as the source code of the SUMO YonoArc block.</p> <h2 id="conclusion">Conclusion</h2> <p>To sum up, we demonstrated the process of integrating SUMO in Yonohub and using it that shows the capability of SUMO for traffic simulation in addition to Yonohub’s computational power as well as the encapsulation concept in YonoArc.</p> <h2 id="about-yonohub">About Yonohub</h2> <blockquote> <p>Yonohub is a web-based cloud system for development, evaluation, integration, and deployment of complex systems, including Artificial Intelligence, Autonomous Driving, and Robotics. Yonohub features a drag-and-drop tool to build complex systems, a marketplace to share and monetize blocks, a builder for custom development environments, and much more. YonoHub can be deployed on-premises and on-cloud.</p> </blockquote> <p>Get $25 free credits when you sign up now. For researchers and labs, contact us to learn more about Yonohub sponsorship options.</p> <p>If you liked this article, please consider following us on Twitter at <a href="https://twitter.com/YonoHub">@yonohub</a>, <a href="mailto:info@yonohub.com">email us directly</a>, or<a href="https://www.linkedin.com/showcase/yonohub"> find us on LinkedIn</a>. I’d love to hear from you if I can help you or your team with how to use Yonohub.</p> <h2 id="reference">Reference</h2> <p>[1] <a href="https://sumo.dlr.de/docs/">https://sumo.dlr.de/docs/</a></p> <p>[2] <a href="https://www.dlr.de/ts/en/desktopdefault.aspx/tabid-9883/16931_read-41000/">https://www.dlr.de/ts/en/desktopdefault.aspx/tabid-9883/16931_read-41000/</a></p> <p>[3] <a href="https://verkehrsforschung.dlr.de/de/news/verkehr-basteln-ganz-einfach-mit-sumo">https://verkehrsforschung.dlr.de/de/news/verkehr-basteln-ganz-einfach-mit-sumo</a></p> <p>[4] <a href="https://yonohub.com/">https://yonohub.com/</a></p>]]></content><author><name></name></author><category term="Yonohub"/><category term="SUMO"/><summary type="html"><![CDATA[Intelligent Transportation System is an essential building block of full autonomy on the roads. To have a safe, sustainable, and efficient autonomous system in our daily life, a complex traffic…]]></summary></entry><entry><title type="html">Part 1 - Introducing TensorFlow Datasets in YonoHub Suit — Image Classification with YonoHub &amp;amp; Tensorflow V2.0 Series</title><link href="ahmedhendawy.de/blog/2020/part1-imageclassification/" rel="alternate" type="text/html" title="Part 1 - Introducing TensorFlow Datasets in YonoHub Suit — Image Classification with YonoHub &amp;amp; Tensorflow V2.0 Series"/><published>2020-04-29T16:40:16+00:00</published><updated>2020-04-29T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2020/part1-imageclassification</id><content type="html" xml:base="ahmedhendawy.de/blog/2020/part1-imageclassification/"><![CDATA[<p align="center"> <img src="https://miro.medium.com/max/700/1*LXoagPOK5PvBzt_ni_8d-A.jpeg" width="800px" title="Tensorflow Series" alt="Tensorflow Series"/> </p> <p>Over the past few years, researchers struggled to find suitable datasets that fit well in their applications. Recently, we were hit by a data storm which enriches our pockets with plenty of datasets which make the job done. However, such a storm has a double-sided effect as we consume a painful time writing different scripts to extract and manipulate these data.</p> <p>On February 26, 2019, Tensorflow had announced <a href="https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html">Tensorflow Datasets </a>(<a href="https://github.com/tensorflow/datase">GitHub </a>) which as I quote,</p> <blockquote> <p>exposes public research datasets as <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><em>tf.data.Datasets</em></a> and as NumPy arrays. It does all the grungy work of fetching the source data and preparing it into a common format on disk, and it uses the <a href="https://www.tensorflow.org/guide/datasets"><em>tf.data API</em></a> to build high-performance input pipelines, which are TensorFlow 2.0-ready and can be used with <em>tf.keras</em> models.</p> </blockquote> <p>Thus, we have a usable high-level layer we can use to call many datasets in a fast way. But can we do better? Can we have an extra layer that even remove the process of writing a line of code?</p> <p><a href="https://yonohub.com/yonoarc/">YonoArc</a> in the <a href="https://yonohub.com/">YonoHub</a> platform provides such a visual programming tool that you easily pick and play some pre-implemented blocks. Your task is to choose which dataset you prefer. But what is the YonoHub platform?</p> <blockquote> <p>Yonohub is the first cloud-based system for designing, sharing, and evaluating complex systems, such as Autonomous Vehicles, ADAS, and Robotics. Yonohub features a drag-and-drop tool to build complex systems consisting of many blocks, a marketplace to share and monetize blocks, a builder for custom environments, and much more.</p> </blockquote> <p>In this series of tutorials, we will go through a deep learning journey, especially <strong>Image Classification</strong>, starting from streaming a dataset till the deployment. The tutorials cover how to use Tensorflow in YonoHub by using the blocks offered within YonoArc.</p> <p>In this tutorial, we will see how to use the Tensorflow Datasets Player block (Image Classification category) within YonoArc. Covering the visualization of over 50 different image classification datasets streamed from a single block. We will use some utils blocks from the OpenCV package in YonoArc.</p> <h2 id="tensorflow-datasets-in-yonoarc">Tensorflow Datasets in YonoArc</h2> <p>Just a few steps and you run your first pipeline of YonoArc blocks. First of all, you need to <a href="https://app.yonohub.com/">signup</a> in YonoHub. After signing in, you will have some core Apps, in the Main View, which you can check them later one by one using the aid of the official <a href="https://yonohub.com/">website</a>.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2004/1*M88cYQCoIT2-JGAD0BhyeA.png" width="800px" title="Main View" alt="Main View"/> </p> <p align="center"> Main View </p> <p>Let’s focus on YonoArc but first, you will need to freely purchase some necessary blocks. To do so you need to click on <a href="https://yonohub.com/yonostore/">YonoStore</a>,</p> <blockquote> <p>YonoStore is a marketplace for the state-of-the-art blocks, datasets, and ready-made runtime environments. YonoStore is fully accessible by both the online users and the users of any on-premises Yonohub, while still protecting everyone’s intellectual property.</p> </blockquote> <p>Follow YonoStore <a href="https://docs.yonohub.com/docs/yonohub/yonostore/">documentation</a> to purchase any product. For our tutorial, search for the following blocks,</p> <ul> <li> <p><a href="https://store.yonohub.com/product/image-classification-player/">Image Classification TFDS Player</a></p> </li> <li> <p><a href="https://store.yonohub.com/product/draw-label/">OpenCV Draw Label</a></p> </li> </ul> <p>After purchasing, let’s create a pipeline!</p> <h3 id="visualize-tensorflow-datasets">Visualize Tensorflow Datasets</h3> <p>In the next clip, we demonstrate how to create a pipeline and launch it. This pipeline aims to investigate the usage of the Image Classification TFDS Player by visualization various numbers of datasets for image classification.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=-yspgvzk1Y8"><img src="http://img.youtube.com/vi/-yspgvzk1Y8/0.jpg" width="800px" title="Visualize Tensorflow Datasets" alt="Visualize Tensorflow Datasets"/></a> </p> <p align="center"> Visualize Tensorflow Datasets </p> <p>The clip shows the advantage of using YonoArc for datasets streaming. We visualized different datasets like beans and cifar10. Furthermore, the frame rate of the streaming has been changed in live mode. You do not need to terminate the full pipeline for that. But can we do more?</p> <h3 id="preprocess-and-draw-labels">Preprocess and Draw Labels</h3> <p>We need to interact with raw images as well as the labels by doing some preprocessing to prepare the training process for such a dataset. In the next clip, we illustrate how to create a custom block to simply resize the streamed images using <em>cv2.resize</em> function in python. Moreover, we draw the streamed labels on top of the resized image using the OpenCV Draw Label block we purchased early.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=XQAEguGxWrk"><img src="http://img.youtube.com/vi/XQAEguGxWrk/0.jpg" width="800px" title="Preprocess and Draw Labels" alt="Preprocess and Draw Labels"/></a> </p> <p align="center"> Preprocess and Draw Labels </p> <p>Now you implement a full pipeline that streams, preprocesses, annotates, and visualizes the beans dataset. You implement a custom block to resize images.</p> <p>To learn more about creating blocks in YonoArc, check the <a href="https://docs.yonohub.com/docs/yonohub/yonoarc/creating-yonoarc-blocks/">docs</a>. You can check the article’s pipeline as well as the source codes for all the blocks used in this <a href="https://github.com/YonoHub/Introducing-TensorFlow-Datasets-in-YonoHub-Suit.git">repository</a>. Furthermore, you can purchase the <a href="https://store.yonohub.com/product/resize/">OpenCV Resize</a> block from YonoStore, it contains the same functionality as the one you implemented but with more features.</p> <h2 id="conclusion">Conclusion</h2> <p>Finally, you are ready to use the pipeline for training. In the next article, we will build a CNN model to classify the cifar10 dataset’s classes. The model will be implemented using Tensorflow V2.0 and encapsulated as a block in YonoArc. It’s easy to try out Yonohub. New users receive $25 free credits. Sign up on <a href="https://yonohub.com/">Yonohub</a>!</p> <h2 id="reference">Reference</h2> <p>[1] <a href="https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html">https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html</a></p> <p>[2] <a href="https://www.tensorflow.org/datasets/catalog/overview">https://www.tensorflow.org/datasets/catalog/overview</a></p> <p>[3] <a href="https://yonohub.com/">https://yonohub.com</a></p> <p>[4] <a href="https://docs.yonohub.com/">https://docs.yonohub.com</a></p> <h2 id="how-to-use-the-arc-file">How to Use the arc file?</h2> <p>You can follow the following steps to use the arc file of the demo directly in YonoArc and achieve the same results,</p> <ul> <li>Freely purchase all the required blocks from YonoStore.</li> <li>Clone the repository and upload the arc file to your YonoDrive.</li> <li>Click on the YonoArc application.</li> <li>Click File, then Open….</li> <li>Browse to the location of the saved arc file and select it.</li> </ul> <p>Now you have the same pipeline above, you can follow the rest of the tutorial to replicate the results.</p>]]></content><author><name></name></author><category term="Yonohub"/><category term="Image_Classification"/><category term="Tensorflow"/><summary type="html"><![CDATA[TensorFlow Datasets is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks. YonoHub encapsulates the datasets in the form of blocks.]]></summary></entry><entry><title type="html">Using YonoHub to Participate in the nuScenes Tracking Challenge</title><link href="ahmedhendawy.de/blog/2019/nuscenes_tracking/" rel="alternate" type="text/html" title="Using YonoHub to Participate in the nuScenes Tracking Challenge"/><published>2019-12-02T16:40:16+00:00</published><updated>2019-12-02T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2019/nuscenes_tracking</id><content type="html" xml:base="ahmedhendawy.de/blog/2019/nuscenes_tracking/"><![CDATA[<p align="center"> <a href="https://www.youtube.com/watch?v=6qslqVRR2kM"><img src="http://img.youtube.com/vi/6qslqVRR2kM/0.jpg" width="800px" title="nuScenes Package Overview" alt="nuScenes Package Overview"/></a> </p> <p align="center"> nuScenes Package Overview </p> <p>A large-scale dataset, which is called <a href="https://www.nuscenes.org">nuScenes</a>, was published by Aptiv Autonomous Mobility (formerly nuTonomy) on the 27th of March 2019, to help the autonomous driving community during the development, and evaluation stages. nuScenes dataset is one of the most promising datasets which facilitate the developing of new autonomous driving algorithms for 3D object detection and 3D object tracking as well.</p> <p>Consequently, the nuScenes team organized a <a href="https://www.nuscenes.org/object-detection?externalData=all&amp;mapData=all&amp;modalities=Any">3D Object Detection Challenge</a> at the last CVPR conference. This challenge aimed to motivate the developers, in the field of the autonomous driving, to benchmark their 3D object detections algorithm against the nuScenes dataset.</p> <p>After the success of the 3D Object Detection challenge, another challenge is recently organized for <a href="https://www.nuscenes.org/tracking?externalData=all&amp;mapData=all&amp;modalities=Any">3D Object Tracking</a>. It will be held during the next NIPS conference.</p> <p>At the same time, the <a href="https://yonohub.com/">YonoHub</a> team has created a nuScenes package that can be used inside the YonoHub platform. The package is in the form of blocks that encapsulate specific functions. The package is used within the main YonoHub application, <a href="https://yonohub.com/yonoarc/">YonoArc</a>. In addition, the nuScenes dataset is available as an extracted version on this cloud-based platform, you do not need to download the dataset with this huge size. You can freely purchase the dataset from <a href="https://store.yonohub.com">YonoStore</a> as well as all the nuScenes package blocks.</p> <p>Accordingly, the nuScenes package became a new method to benchmark your algorithm against the nuScenes dataset for the upcoming nuScenes Tracking Challenge. Just by drag and drop some blocks, you will be able to interact with the dataset.</p> <p>In addition to the blocks, Yonohub provides sponsored credits (up to $1000) to work on nuScenes 3D Tracking Challenge. Apply using <a href="https://yonohub.com/nuscenes-package-and-sponsorship/">this form</a> to receive the sponsored credits directly in your account.</p> <p>Let’s discuss how can you start working with the package!</p> <h2 id="how-does-it-work">How does it work?</h2> <p>As a first step, you need to get the nuScenes dataset as well as the nuScenes blocks for YonoArc development. YonoStore is the place where you can purchase a dataset, block, or even environment. There are different datasets other than nuScenes, for example, Kitti Dataset, Comma.ai, etc..</p> <p>After purchasing the assets, you will find the nuScenes Dataset in your YonoStoreDataset folder, however, the nuScenes blocks in your YonoArc blocks.</p> <p>The nuScenes package enriches of different kinds of blocks. The last update of the package consists of 10 blocks as listed below,</p> <ul> <li> <p><strong>Dataset Player</strong>: used to stream all the sample data for all kinds of sensors as well as the metadata of the nuScenes dataset.</p> </li> <li> <p><strong>Sample Annotations to Eval Boxes</strong>: used to convert the sample annotation box format to eval box format.</p> </li> <li> <p><strong>Boxes Frame Transformer</strong>: used to transform the reference frame of the boxes to another one.</p> </li> <li> <p><strong>Lidar Point Cloud Converter</strong>: <em>**</em>used to convert the reference frame of the lidar point cloud from nuScenes lidar frame to Kitti lidar frame or vice versa.</p> </li> <li> <p><strong>Eval Boxes Preprocessing</strong>: <em>**</em>used to perform the preprocessing over the eval boxes before the evaluation process.</p> </li> <li> <p><strong>Predictions Appender</strong>: used to append/ batch all the predicted boxes as a JSON file for submission.</p> </li> <li> <p><strong>Predictions Loader</strong>: used to load the results saved as JSON file for the evaluation/visualization process.</p> </li> <li> <p><strong>Tracking Benchmark</strong>: used to perform the tracking evaluation bench-marking of the predicted boxes against the ground truth boxes from the dataset.</p> </li> <li> <p><strong>Boxes to Markers</strong>: used to convert the boxes to markers format for proper visualization in <a href="http://wiki.ros.org/rviz">Rviz</a> (Purchase Rviz block from YonoStore).</p> </li> <li> <p><strong>Draw 3D Boxes</strong>: used to draw the boxes over one of the camera output images.</p> </li> </ul> <p>NOTE: To learn more about the block’s settings, functionality, as well as input/output ports, check the description associated with the item in YonoStore or through the Help tab in the block’s settings in YonoArc.</p> <p>After reading the brief description of each block in the package, we need to illustrate how to use it in YonoArc. We can check the following series of tutorial videos which are very useful to understand the bench-marking cycle in YonoArc. For a more detailed description regarding the following tutorials, please read the corresponding tutorial in the <a href="https://docs.yonohub.com/docs/yonohub/nuscenes-package/">docs</a>.</p> <h3 id="getting-started-with-nuscenes">Getting Started with nuScenes</h3> <p>In this tutorial, we learn how to interact with the nuScenes dataset through the Dataset Player block. Streaming the raw images of one of the dataset’s camera sensors and visualize it using the Dashboard in YonoArc.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=jLYU5-gqp9Y"><img src="http://img.youtube.com/vi/jLYU5-gqp9Y/0.jpg" width="800px" title="Tutorial 1: Getting Started with nuScenes " alt="Tutorial 1: Getting Started with nuScenes"/></a> </p> <p align="center"> Tutorial 1: Getting Started with nuScenes </p> <h3 id="visualize-the-nuscenes-dataset">Visualize the nuScenes Dataset</h3> <p>In this tutorial, we deal with the different visualization methods, which are offered through the nuScenes package in YonoArc. At the end of this tutorial, you learn how to,</p> <ul> <li> <p>Draw the 3D Ground Truth Boxes on the raw images captured by Camera sensors.</p> </li> <li> <p>Visualize the Radar/Lidar point cloud in Rviz with a different number of sweeps.</p> </li> <li> <p>Convert the 3D Ground Truth Boxes to <a href="http://docs.ros.org/melodic/api/visualization_msgs/html/msg/Marker.html">Markers</a> to visualize it in Rviz on the top of the Lidar point cloud.</p> </li> </ul> <p align="center"> <a href="https://www.youtube.com/watch?v=XWoKT2rp1ck"><img src="http://img.youtube.com/vi/XWoKT2rp1ck/0.jpg" width="800px" title="Tutorial 2: Visualize the nuScenes Dataset" alt="Tutorial 2: Visualize the nuScenes Dataset"/></a> </p> <p align="center"> Tutorial 2: Visualize the nuScenes Dataset </p> <h3 id="predict-and-save-your-results">Predict and Save Your Results</h3> <p>In this tutorial, you take your first steps regarding the involvement in the nuScenes Tracking Challenge. At the end of this tutorial, you learn how to,</p> <ul> <li> <p>Load the detection results of one of the <a href="https://www.nuscenes.org/tracking/#baselines">supported algorithms</a> by the nuScenes Challenge, for example, MEGVII.</p> </li> <li> <p>Use a pre-implemented tracking algorithm like AB3DMOT. You can develop your own algorithm as a YonoArc block using <a href="https://docs.yonohub.com/docs/yonohub/tutorials/custom-python3-block/">this</a> tutorial.</p> </li> <li> <p>Append your results after doing the preprocessing stage as a JSON file with the same nuScenes Tracking Challenge format.</p> </li> </ul> <p>NOTE: You can purchase the AB3DMOT tracking algorithm from YonoStore.</p> <p align="center"> <a href="https://www.youtube.com/watch?v=ZZEltqiXgao"><img src="http://img.youtube.com/vi/ZZEltqiXgao/0.jpg" width="800px" title="Tutorial 3: Predict and Save Your Results " alt="Tutorial 3: Predict and Save Your Results "/></a> </p> <p align="center"> Tutorial 3: Predict and Save Your Results </p> <h3 id="benchmark-your-tracking-algorithm">Benchmark Your Tracking Algorithm</h3> <p>In this final tutorial, we construct an evaluation/bench-marking loop for the challenge. At the end of this tutorial, you learn how to,</p> <ul> <li> <p>Load your own tracking results from the previous tutorial.</p> </li> <li> <p>Benchmark your algorithm against the nuScenes Dataset.</p> </li> </ul> <p align="center"> <a href="https://www.youtube.com/watch?v=nCp-T6JIynw"><img src="http://img.youtube.com/vi/nCp-T6JIynw/0.jpg" width="800px" title="Tutorial 4: Benchmark Your Algorithm" alt="Tutorial 4: Benchmark Your Algorithm"/></a></p> <p align="center"> Tutorial 4: Benchmark Your Algorithm </p> <p>NOTE: You can find all the previous YonoArc pipelines in <a href="https://gitlab.yonohub.com/YonoTeam/nuscenes_package_tutorials">this</a> repository.</p> <h2 id="conclusion">Conclusion</h2> <p>Finally, you are ready to participate in the nuScenes Tracking Challenge with an easy-to-use drag and drop interface. You do not need to download this huge sized dataset on your local machine, just freely purchase the dataset and you get it in no time. All the development can be done on YonoHub using different machine capabilities.</p> <p>It’s easy to try out Yonohub. New users receive $50 free credits. For nuScenes Challenges, Yonohub provides sponsored credits (up to $1000) to work on nuScenes 3D Tracking Challenge. Apply using <a href="https://yonohub.com/nuscenes-package-and-sponsorship/">this form</a> to receive the sponsored credits directly in your account. Sign up on Yonohub!</p>]]></content><author><name></name></author><category term="Yonohub"/><category term="nuScenes"/><category term="Object_Tracking"/><summary type="html"><![CDATA[The nuScenes package, in YonoArc, facilitates the participation in the nuScenes Tracking Challenge.]]></summary></entry><entry><title type="html">Image Classification using Torchvision Pre-trained Models in a Single YonoArc Block</title><link href="ahmedhendawy.de/blog/2019/Torchvision/" rel="alternate" type="text/html" title="Image Classification using Torchvision Pre-trained Models in a Single YonoArc Block"/><published>2019-09-25T16:40:16+00:00</published><updated>2019-09-25T16:40:16+00:00</updated><id>ahmedhendawy.de/blog/2019/Torchvision</id><content type="html" xml:base="ahmedhendawy.de/blog/2019/Torchvision/"><![CDATA[<p align="center"> <img src="https://cdn-images-1.medium.com/max/2724/1*4qKMqWYD401zdUPsptB8lA.jpeg" width="800px" title="Overview" alt="Overview"/> </p> <p>Recently, I decided to learn how to implement machine learning (ML) algorithms, especially in the computer vision field, using the well-known package <a href="https://pytorch.org"><strong>Pytorch</strong></a>. One of the useful resources, to break the ice with such a powerful ML package, is the <a href="https://www.learnopencv.com/learn-pytorch/"><strong>PyTorch for Beginners</strong></a> series of tutorials offered by <a href="https://www.learnopencv.com"><strong>Learn OpenCV</strong></a>. The series contains some core topics that should be understood by the computer vision developers who are using Pytorch.</p> <p>I follow the second tutorial with the title: <a href="https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/"><strong>PyTorch for Beginners: Image Classification using Pre-trained models</strong></a>. I am surprised by the amount of the architectures supported by the <a href="https://pytorch.org/docs/stable/torchvision/index.html"><strong>Torchvision</strong></a> module for image classification.</p> <blockquote> <p><strong>Torchvision</strong> package consists of popular datasets, model architectures, and common image transformations for computer vision. Basically, if you are into Computer Vision and using PyTorch, Torchvision will be of great help!</p> </blockquote> <p>I thought it will be a beneficial idea to use YonoArc App in the <a href="https://yonohub.com"><strong>YonoHub</strong></a> platform to encapsulate all the supported image classification architectures, by torchvision package, in a single block. The developers will have the facility of using only one YonoArc block to evaluate and benchmark the different performances of the well-known classifiers with its pre-trained weights.</p> <blockquote> <p><strong>YonoHub</strong> is the first cloud-based system for designing, sharing, and evaluating complex systems, such as Autonomous Vehicles, ADAS, and Robotics. Yonohub features a drag-and-drop tool to build complex systems consisting of many blocks, a marketplace to share and monetize blocks, a builder for custom environments, and much more.</p> </blockquote> <h2 id="how-does-the-block-work">How does the block work?</h2> <p>To have a good hand-on example, I use in the following demo the nuScenes dataset which can be streamed in the YonoArc pipeline using its supported player block. For more information about nuScenes-YonoArc package, you can read my previous article <a href="https://medium.com/@ahmedmagdyattia1996/nuscenes-using-yonohub-to-develop-evaluate-and-benchmark-over-the-cloud-94c06685c168">here</a>.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2592/1*Otk5RtkP7NVDhZ-qYBtUAA.jpeg" width="800px" title="Image Classification Demo" alt="Image Classification Demo"/> </p> <p align="center"> Image Classification Demo </p> <p><strong>Torchvision Image Classifier</strong> YonoArc block has one input which intuitively of type Image. The source of the input images is the <strong>nuScenes Dataset Player</strong> as shown above. On the other hand, the block has two different outputs. The first output port is an image after writing the classes that the model is certain about. The other port is the classes associated with its confidence that the model predicted. The type of this output port is a custom ROS message which is included in the <a href="https://gitlab.yonohub.com/AhmedHendawy/perception-msgs"><strong>perception_msgs</strong></a> package I implemented for interacting with the YonoArc blocks in the field of computer vision.</p> <p>The image classifier block has a property of choosing the model which you want to evaluate. As shown below, you have a drop list, under the title <strong>Model</strong> **Options, **with 31 different image classification models. You can reach this drop list by clicking on the setting icon of the block.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2594/1*fHJC3qdYWbYsa7ae0U35Cg.png" width="800px" title="Image Classification Model Options" alt="Image Classification Model Options"/> </p> <p align="center"> Image Classification Model Options </p> <p>One great thing about YonoArc is that each block can use different resource models in the same pipeline. For example, the image classifier block uses GPU as a resource model. However, an eight-core CPU is the resource model of both the nuScenes player and the video viewer blocks.</p> <h2 id="source-code">Source Code</h2> <p>During the implementation of the presented block, I make use of the LearnOpenCV tutorial code. I manipulate it to be general and suitable for the YonoArc blocks code structure. <a href="https://docs.yonohub.com"><strong>YonoHub Docs</strong></a> is very useful to understand how to make a python 3 API YonoArc block. You can read this <a href="https://docs.yonohub.com/docs/yonohub/yonoarc/yonoarc-python3-api/">part</a> of the docs to understand the changes happen to the tutorial code. You can check the source code below,</p> <script src="https://gist.github.com/AhmedMagdyHendawy/a002b062dcde6d3111257019b62dfad4.js"></script> <h2 id="results">Results</h2> <p>After running the above pipeline, the below results have been produced. AlexNet model is chosen to predict the shown estimated classes.</p> <p align="center"> <img src="https://cdn-images-1.medium.com/max/2000/1*-lvQb1kJ4zxnLdBR3CTnkw.gif" width="800px" title="Image Classification Results" alt="Image Classification Results"/> </p> <p align="center"> Image Classification Results </p> <p>Finally, I tried to demonstrate how to use YonoArc App in the YonoHub platform to implement the image classification tutorial by Learn OpenCV. Merging the benefits of both Pytorch, especially Torchvision, and YonoArc App to have an encapsulated image classification library of models in the shape of a block. You can purchase the <strong>Torchvision Image Classifier</strong> block for free, through <a href="https://store.yonohub.com/product/torchvision-image-classifier/"><strong>YonoStore</strong></a>, and try the different models available without dealing with the code details. In addition, you can clone the above source code from <a href="https://gitlab.yonohub.com/AhmedHendawy/torchvision_image_classifier">here</a> to reuse it for such blocks. Follow the steps in the referred <a href="https://docs.yonohub.com/docs/yonohub/tutorials/custom-python3-block/">tutorial</a> to learn how to implement a YonoArc block.</p> <p>Next, I will work on the semantic segmentation tutorial to have a similar reusable YonoArc block. It’s easy to try out Yonohub. New users receive $25 free credits. Sign up on <a href="https://yonohub.com/">Yonohub</a>!</p>]]></content><author><name></name></author><category term="Yonohub"/><category term="Image_Classification"/><category term="Torchvision"/><summary type="html"><![CDATA[YonoHub series of tutorials which mimic the PyTorch for Beginners tutorials in a single reusable block.]]></summary></entry></feed>